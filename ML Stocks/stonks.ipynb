{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close/Last</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12/09/2024</td>\n",
       "      <td>604.6800</td>\n",
       "      <td>34742740</td>\n",
       "      <td>607.69</td>\n",
       "      <td>607.860</td>\n",
       "      <td>604.080</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12/06/2024</td>\n",
       "      <td>607.8100</td>\n",
       "      <td>31241550</td>\n",
       "      <td>607.44</td>\n",
       "      <td>609.070</td>\n",
       "      <td>607.020</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12/05/2024</td>\n",
       "      <td>606.6600</td>\n",
       "      <td>28762180</td>\n",
       "      <td>607.66</td>\n",
       "      <td>608.480</td>\n",
       "      <td>606.305</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12/04/2024</td>\n",
       "      <td>607.6600</td>\n",
       "      <td>42787560</td>\n",
       "      <td>605.63</td>\n",
       "      <td>607.910</td>\n",
       "      <td>604.950</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12/03/2024</td>\n",
       "      <td>603.9100</td>\n",
       "      <td>26906630</td>\n",
       "      <td>603.39</td>\n",
       "      <td>604.160</td>\n",
       "      <td>602.341</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>2511</td>\n",
       "      <td>12/16/2014</td>\n",
       "      <td>197.9100</td>\n",
       "      <td>258867900</td>\n",
       "      <td>198.58</td>\n",
       "      <td>202.395</td>\n",
       "      <td>197.860</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>2512</td>\n",
       "      <td>12/15/2014</td>\n",
       "      <td>199.5100</td>\n",
       "      <td>188920400</td>\n",
       "      <td>201.98</td>\n",
       "      <td>202.530</td>\n",
       "      <td>198.780</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>2513</td>\n",
       "      <td>12/12/2014</td>\n",
       "      <td>200.8900</td>\n",
       "      <td>201983200</td>\n",
       "      <td>202.64</td>\n",
       "      <td>203.819</td>\n",
       "      <td>200.850</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>2514</td>\n",
       "      <td>12/11/2014</td>\n",
       "      <td>204.1900</td>\n",
       "      <td>158542800</td>\n",
       "      <td>203.88</td>\n",
       "      <td>206.190</td>\n",
       "      <td>203.710</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>2515</td>\n",
       "      <td>12/10/2014</td>\n",
       "      <td>203.1601</td>\n",
       "      <td>159140400</td>\n",
       "      <td>205.91</td>\n",
       "      <td>205.980</td>\n",
       "      <td>202.930</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2516 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index        Date  Close/Last     Volume    Open     High      Low  \\\n",
       "0         0  12/09/2024    604.6800   34742740  607.69  607.860  604.080   \n",
       "1         1  12/06/2024    607.8100   31241550  607.44  609.070  607.020   \n",
       "2         2  12/05/2024    606.6600   28762180  607.66  608.480  606.305   \n",
       "3         3  12/04/2024    607.6600   42787560  605.63  607.910  604.950   \n",
       "4         4  12/03/2024    603.9100   26906630  603.39  604.160  602.341   \n",
       "...     ...         ...         ...        ...     ...      ...      ...   \n",
       "2511   2511  12/16/2014    197.9100  258867900  198.58  202.395  197.860   \n",
       "2512   2512  12/15/2014    199.5100  188920400  201.98  202.530  198.780   \n",
       "2513   2513  12/12/2014    200.8900  201983200  202.64  203.819  200.850   \n",
       "2514   2514  12/11/2014    204.1900  158542800  203.88  206.190  203.710   \n",
       "2515   2515  12/10/2014    203.1601  159140400  205.91  205.980  202.930   \n",
       "\n",
       "      day_of_week  day  month  year  \n",
       "0               0    9     12  2024  \n",
       "1               4    6     12  2024  \n",
       "2               3    5     12  2024  \n",
       "3               2    4     12  2024  \n",
       "4               1    3     12  2024  \n",
       "...           ...  ...    ...   ...  \n",
       "2511            1   16     12  2014  \n",
       "2512            0   15     12  2014  \n",
       "2513            4   12     12  2014  \n",
       "2514            3   11     12  2014  \n",
       "2515            2   10     12  2014  \n",
       "\n",
       "[2516 rows x 11 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, recall_score, precision_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "df = pd.read_csv(\"SPY.csv\")\n",
    "df['day_of_week'] = pd.to_datetime(df['Date']).dt.dayofweek\n",
    "df['day'] = pd.to_datetime(df['Date']).dt.day\n",
    "df['month'] = pd.to_datetime(df['Date']).dt.month\n",
    "df['year'] = pd.to_datetime(df['Date']).dt.year\n",
    "df = df.reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_before(df, days):\n",
    "    all_prev_data = []  # List to store all the previous data\n",
    "    \n",
    "    for day in range(1, days + 1):\n",
    "        def prior(row):\n",
    "            # Ensure we do not go out of bounds (row.name + day must be within the DataFrame index)\n",
    "            if row.name + day < len(df):\n",
    "                prior_row = df.iloc[row.name + day]\n",
    "                return pd.Series({\n",
    "                    f\"prev{day}Close/Last\": prior_row[\"Close/Last\"],\n",
    "                    f\"prev{day}Volume\": prior_row[\"Volume\"],\n",
    "                    f\"prev{day}Open\": prior_row[\"Open\"],\n",
    "                    f\"prev{day}High\": prior_row[\"High\"],\n",
    "                    f\"prev{day}Low\": prior_row[\"Low\"]\n",
    "                })\n",
    "            else:\n",
    "                # Return NaN for out-of-bounds cases\n",
    "                return pd.Series({\n",
    "                    f\"prev{day}Close/Last\": None,\n",
    "                    f\"prev{day}Volume\": None,\n",
    "                    f\"prev{day}Open\": None,\n",
    "                    f\"prev{day}High\": None,\n",
    "                    f\"prev{day}Low\": None\n",
    "                })\n",
    "\n",
    "        # Apply the prior function to each row and create new columns\n",
    "        prev_data = df.apply(prior, axis=1)\n",
    "        all_prev_data.append(prev_data)  # Append the data for this day\n",
    "\n",
    "    # Concatenate all the new columns at once\n",
    "    df = pd.concat([df] + all_prev_data, axis=1)\n",
    "\n",
    "    # Drop the last 'days' rows to remove rows with NaN values\n",
    "    df = df.dropna(axis=0, how='any')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Call the function\n",
    "df = days_before(df, 20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close/Last</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>...</th>\n",
       "      <th>prev19Close/Last</th>\n",
       "      <th>prev19Volume</th>\n",
       "      <th>prev19Open</th>\n",
       "      <th>prev19High</th>\n",
       "      <th>prev19Low</th>\n",
       "      <th>prev20Close/Last</th>\n",
       "      <th>prev20Volume</th>\n",
       "      <th>prev20Open</th>\n",
       "      <th>prev20High</th>\n",
       "      <th>prev20Low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12/09/2024</td>\n",
       "      <td>604.6800</td>\n",
       "      <td>34742740</td>\n",
       "      <td>607.69</td>\n",
       "      <td>607.86</td>\n",
       "      <td>604.080</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>598.76</td>\n",
       "      <td>37586770.0</td>\n",
       "      <td>599.81</td>\n",
       "      <td>600.170</td>\n",
       "      <td>597.0000</td>\n",
       "      <td>598.1900</td>\n",
       "      <td>46444890.0</td>\n",
       "      <td>596.17</td>\n",
       "      <td>599.640</td>\n",
       "      <td>596.1650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12/06/2024</td>\n",
       "      <td>607.8100</td>\n",
       "      <td>31241550</td>\n",
       "      <td>607.44</td>\n",
       "      <td>609.07</td>\n",
       "      <td>607.020</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>598.19</td>\n",
       "      <td>46444890.0</td>\n",
       "      <td>596.17</td>\n",
       "      <td>599.640</td>\n",
       "      <td>596.1650</td>\n",
       "      <td>595.6100</td>\n",
       "      <td>47233210.0</td>\n",
       "      <td>593.08</td>\n",
       "      <td>596.650</td>\n",
       "      <td>592.9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12/05/2024</td>\n",
       "      <td>606.6600</td>\n",
       "      <td>28762180</td>\n",
       "      <td>607.66</td>\n",
       "      <td>608.48</td>\n",
       "      <td>606.305</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>595.61</td>\n",
       "      <td>47233210.0</td>\n",
       "      <td>593.08</td>\n",
       "      <td>596.650</td>\n",
       "      <td>592.9999</td>\n",
       "      <td>591.0400</td>\n",
       "      <td>68181970.0</td>\n",
       "      <td>589.20</td>\n",
       "      <td>591.930</td>\n",
       "      <td>585.3900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12/04/2024</td>\n",
       "      <td>607.6600</td>\n",
       "      <td>42787560</td>\n",
       "      <td>605.63</td>\n",
       "      <td>607.91</td>\n",
       "      <td>604.950</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>591.04</td>\n",
       "      <td>68181970.0</td>\n",
       "      <td>589.20</td>\n",
       "      <td>591.930</td>\n",
       "      <td>585.3900</td>\n",
       "      <td>576.7000</td>\n",
       "      <td>39478320.0</td>\n",
       "      <td>570.74</td>\n",
       "      <td>576.740</td>\n",
       "      <td>570.5200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12/03/2024</td>\n",
       "      <td>603.9100</td>\n",
       "      <td>26906630</td>\n",
       "      <td>603.39</td>\n",
       "      <td>604.16</td>\n",
       "      <td>602.341</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>576.70</td>\n",
       "      <td>39478320.0</td>\n",
       "      <td>570.74</td>\n",
       "      <td>576.740</td>\n",
       "      <td>570.5200</td>\n",
       "      <td>569.8100</td>\n",
       "      <td>38216980.0</td>\n",
       "      <td>571.18</td>\n",
       "      <td>572.500</td>\n",
       "      <td>567.8900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2491</th>\n",
       "      <td>2491</td>\n",
       "      <td>01/15/2015</td>\n",
       "      <td>199.0199</td>\n",
       "      <td>175383200</td>\n",
       "      <td>201.63</td>\n",
       "      <td>202.01</td>\n",
       "      <td>198.880</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>201.79</td>\n",
       "      <td>252458700.0</td>\n",
       "      <td>198.44</td>\n",
       "      <td>202.340</td>\n",
       "      <td>198.2900</td>\n",
       "      <td>197.9100</td>\n",
       "      <td>258867900.0</td>\n",
       "      <td>198.58</td>\n",
       "      <td>202.395</td>\n",
       "      <td>197.8600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2492</th>\n",
       "      <td>2492</td>\n",
       "      <td>01/14/2015</td>\n",
       "      <td>200.8600</td>\n",
       "      <td>192551700</td>\n",
       "      <td>199.65</td>\n",
       "      <td>201.10</td>\n",
       "      <td>198.570</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>197.91</td>\n",
       "      <td>258867900.0</td>\n",
       "      <td>198.58</td>\n",
       "      <td>202.395</td>\n",
       "      <td>197.8600</td>\n",
       "      <td>199.5100</td>\n",
       "      <td>188920400.0</td>\n",
       "      <td>201.98</td>\n",
       "      <td>202.530</td>\n",
       "      <td>198.7800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>2493</td>\n",
       "      <td>01/13/2015</td>\n",
       "      <td>202.0800</td>\n",
       "      <td>214341100</td>\n",
       "      <td>204.12</td>\n",
       "      <td>205.48</td>\n",
       "      <td>200.510</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>199.51</td>\n",
       "      <td>188920400.0</td>\n",
       "      <td>201.98</td>\n",
       "      <td>202.530</td>\n",
       "      <td>198.7800</td>\n",
       "      <td>200.8900</td>\n",
       "      <td>201983200.0</td>\n",
       "      <td>202.64</td>\n",
       "      <td>203.819</td>\n",
       "      <td>200.8500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>2494</td>\n",
       "      <td>01/12/2015</td>\n",
       "      <td>202.6500</td>\n",
       "      <td>144043100</td>\n",
       "      <td>204.41</td>\n",
       "      <td>204.60</td>\n",
       "      <td>201.920</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>200.89</td>\n",
       "      <td>201983200.0</td>\n",
       "      <td>202.64</td>\n",
       "      <td>203.819</td>\n",
       "      <td>200.8500</td>\n",
       "      <td>204.1900</td>\n",
       "      <td>158542800.0</td>\n",
       "      <td>203.88</td>\n",
       "      <td>206.190</td>\n",
       "      <td>203.7100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>2495</td>\n",
       "      <td>01/09/2015</td>\n",
       "      <td>204.2500</td>\n",
       "      <td>157293400</td>\n",
       "      <td>206.40</td>\n",
       "      <td>206.42</td>\n",
       "      <td>203.510</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>204.19</td>\n",
       "      <td>158542800.0</td>\n",
       "      <td>203.88</td>\n",
       "      <td>206.190</td>\n",
       "      <td>203.7100</td>\n",
       "      <td>203.1601</td>\n",
       "      <td>159140400.0</td>\n",
       "      <td>205.91</td>\n",
       "      <td>205.980</td>\n",
       "      <td>202.9300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2496 rows Ã— 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index        Date  Close/Last     Volume    Open    High      Low  \\\n",
       "0         0  12/09/2024    604.6800   34742740  607.69  607.86  604.080   \n",
       "1         1  12/06/2024    607.8100   31241550  607.44  609.07  607.020   \n",
       "2         2  12/05/2024    606.6600   28762180  607.66  608.48  606.305   \n",
       "3         3  12/04/2024    607.6600   42787560  605.63  607.91  604.950   \n",
       "4         4  12/03/2024    603.9100   26906630  603.39  604.16  602.341   \n",
       "...     ...         ...         ...        ...     ...     ...      ...   \n",
       "2491   2491  01/15/2015    199.0199  175383200  201.63  202.01  198.880   \n",
       "2492   2492  01/14/2015    200.8600  192551700  199.65  201.10  198.570   \n",
       "2493   2493  01/13/2015    202.0800  214341100  204.12  205.48  200.510   \n",
       "2494   2494  01/12/2015    202.6500  144043100  204.41  204.60  201.920   \n",
       "2495   2495  01/09/2015    204.2500  157293400  206.40  206.42  203.510   \n",
       "\n",
       "      day_of_week  day  month  ...  prev19Close/Last  prev19Volume  \\\n",
       "0               0    9     12  ...            598.76    37586770.0   \n",
       "1               4    6     12  ...            598.19    46444890.0   \n",
       "2               3    5     12  ...            595.61    47233210.0   \n",
       "3               2    4     12  ...            591.04    68181970.0   \n",
       "4               1    3     12  ...            576.70    39478320.0   \n",
       "...           ...  ...    ...  ...               ...           ...   \n",
       "2491            3   15      1  ...            201.79   252458700.0   \n",
       "2492            2   14      1  ...            197.91   258867900.0   \n",
       "2493            1   13      1  ...            199.51   188920400.0   \n",
       "2494            0   12      1  ...            200.89   201983200.0   \n",
       "2495            4    9      1  ...            204.19   158542800.0   \n",
       "\n",
       "      prev19Open  prev19High  prev19Low  prev20Close/Last  prev20Volume  \\\n",
       "0         599.81     600.170   597.0000          598.1900    46444890.0   \n",
       "1         596.17     599.640   596.1650          595.6100    47233210.0   \n",
       "2         593.08     596.650   592.9999          591.0400    68181970.0   \n",
       "3         589.20     591.930   585.3900          576.7000    39478320.0   \n",
       "4         570.74     576.740   570.5200          569.8100    38216980.0   \n",
       "...          ...         ...        ...               ...           ...   \n",
       "2491      198.44     202.340   198.2900          197.9100   258867900.0   \n",
       "2492      198.58     202.395   197.8600          199.5100   188920400.0   \n",
       "2493      201.98     202.530   198.7800          200.8900   201983200.0   \n",
       "2494      202.64     203.819   200.8500          204.1900   158542800.0   \n",
       "2495      203.88     206.190   203.7100          203.1601   159140400.0   \n",
       "\n",
       "      prev20Open  prev20High  prev20Low  \n",
       "0         596.17     599.640   596.1650  \n",
       "1         593.08     596.650   592.9999  \n",
       "2         589.20     591.930   585.3900  \n",
       "3         570.74     576.740   570.5200  \n",
       "4         571.18     572.500   567.8900  \n",
       "...          ...         ...        ...  \n",
       "2491      198.58     202.395   197.8600  \n",
       "2492      201.98     202.530   198.7800  \n",
       "2493      202.64     203.819   200.8500  \n",
       "2494      203.88     206.190   203.7100  \n",
       "2495      205.91     205.980   202.9300  \n",
       "\n",
       "[2496 rows x 111 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sakug\\AppData\\Local\\Temp\\ipykernel_9696\\4194361028.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"NextDayUp1\"] = df.apply(is_up, axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close/Last</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>prev1Close/Last</th>\n",
       "      <th>...</th>\n",
       "      <th>prev19Close/Last</th>\n",
       "      <th>prev19Volume</th>\n",
       "      <th>prev19Open</th>\n",
       "      <th>prev19High</th>\n",
       "      <th>prev19Low</th>\n",
       "      <th>prev20Close/Last</th>\n",
       "      <th>prev20Volume</th>\n",
       "      <th>prev20Open</th>\n",
       "      <th>prev20High</th>\n",
       "      <th>prev20Low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>475.3100</td>\n",
       "      <td>122283100</td>\n",
       "      <td>476.49</td>\n",
       "      <td>477.030</td>\n",
       "      <td>473.30</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>2023</td>\n",
       "      <td>472.65</td>\n",
       "      <td>...</td>\n",
       "      <td>491.27</td>\n",
       "      <td>61322750.0</td>\n",
       "      <td>487.730</td>\n",
       "      <td>491.415</td>\n",
       "      <td>487.1700</td>\n",
       "      <td>490.8900</td>\n",
       "      <td>58618390.0</td>\n",
       "      <td>490.560</td>\n",
       "      <td>491.620</td>\n",
       "      <td>490.1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>476.6900</td>\n",
       "      <td>77158120</td>\n",
       "      <td>476.88</td>\n",
       "      <td>477.550</td>\n",
       "      <td>476.26</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>2023</td>\n",
       "      <td>475.31</td>\n",
       "      <td>...</td>\n",
       "      <td>487.41</td>\n",
       "      <td>76641610.0</td>\n",
       "      <td>487.590</td>\n",
       "      <td>489.120</td>\n",
       "      <td>486.5400</td>\n",
       "      <td>491.2700</td>\n",
       "      <td>61322750.0</td>\n",
       "      <td>487.730</td>\n",
       "      <td>491.415</td>\n",
       "      <td>487.1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>476.5100</td>\n",
       "      <td>68000310</td>\n",
       "      <td>475.44</td>\n",
       "      <td>476.660</td>\n",
       "      <td>474.89</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>2023</td>\n",
       "      <td>476.69</td>\n",
       "      <td>...</td>\n",
       "      <td>488.03</td>\n",
       "      <td>72524990.0</td>\n",
       "      <td>487.575</td>\n",
       "      <td>488.305</td>\n",
       "      <td>485.3900</td>\n",
       "      <td>487.4100</td>\n",
       "      <td>76641610.0</td>\n",
       "      <td>487.590</td>\n",
       "      <td>489.120</td>\n",
       "      <td>486.5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>475.6500</td>\n",
       "      <td>55386950</td>\n",
       "      <td>474.07</td>\n",
       "      <td>476.580</td>\n",
       "      <td>473.99</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>2023</td>\n",
       "      <td>476.51</td>\n",
       "      <td>...</td>\n",
       "      <td>485.39</td>\n",
       "      <td>81765040.0</td>\n",
       "      <td>487.810</td>\n",
       "      <td>488.770</td>\n",
       "      <td>484.8819</td>\n",
       "      <td>488.0300</td>\n",
       "      <td>72524990.0</td>\n",
       "      <td>487.575</td>\n",
       "      <td>488.305</td>\n",
       "      <td>485.3900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>473.6500</td>\n",
       "      <td>67160420</td>\n",
       "      <td>473.86</td>\n",
       "      <td>475.380</td>\n",
       "      <td>471.70</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>2023</td>\n",
       "      <td>475.65</td>\n",
       "      <td>...</td>\n",
       "      <td>484.86</td>\n",
       "      <td>49945300.0</td>\n",
       "      <td>484.010</td>\n",
       "      <td>485.105</td>\n",
       "      <td>482.8900</td>\n",
       "      <td>485.3900</td>\n",
       "      <td>81765040.0</td>\n",
       "      <td>487.810</td>\n",
       "      <td>488.770</td>\n",
       "      <td>484.8819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>197.9100</td>\n",
       "      <td>258867900</td>\n",
       "      <td>198.58</td>\n",
       "      <td>202.395</td>\n",
       "      <td>197.86</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "      <td>201.79</td>\n",
       "      <td>...</td>\n",
       "      <td>200.86</td>\n",
       "      <td>192551700.0</td>\n",
       "      <td>199.650</td>\n",
       "      <td>201.100</td>\n",
       "      <td>198.5700</td>\n",
       "      <td>199.0199</td>\n",
       "      <td>175383200.0</td>\n",
       "      <td>201.630</td>\n",
       "      <td>202.010</td>\n",
       "      <td>198.8800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>199.5100</td>\n",
       "      <td>188920400</td>\n",
       "      <td>201.98</td>\n",
       "      <td>202.530</td>\n",
       "      <td>198.78</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "      <td>197.91</td>\n",
       "      <td>...</td>\n",
       "      <td>202.08</td>\n",
       "      <td>214341100.0</td>\n",
       "      <td>204.120</td>\n",
       "      <td>205.480</td>\n",
       "      <td>200.5100</td>\n",
       "      <td>200.8600</td>\n",
       "      <td>192551700.0</td>\n",
       "      <td>199.650</td>\n",
       "      <td>201.100</td>\n",
       "      <td>198.5700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>200.8900</td>\n",
       "      <td>201983200</td>\n",
       "      <td>202.64</td>\n",
       "      <td>203.819</td>\n",
       "      <td>200.85</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "      <td>199.51</td>\n",
       "      <td>...</td>\n",
       "      <td>202.65</td>\n",
       "      <td>144043100.0</td>\n",
       "      <td>204.410</td>\n",
       "      <td>204.600</td>\n",
       "      <td>201.9200</td>\n",
       "      <td>202.0800</td>\n",
       "      <td>214341100.0</td>\n",
       "      <td>204.120</td>\n",
       "      <td>205.480</td>\n",
       "      <td>200.5100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>204.1900</td>\n",
       "      <td>158542800</td>\n",
       "      <td>203.88</td>\n",
       "      <td>206.190</td>\n",
       "      <td>203.71</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "      <td>200.89</td>\n",
       "      <td>...</td>\n",
       "      <td>204.25</td>\n",
       "      <td>157293400.0</td>\n",
       "      <td>206.400</td>\n",
       "      <td>206.420</td>\n",
       "      <td>203.5100</td>\n",
       "      <td>202.6500</td>\n",
       "      <td>144043100.0</td>\n",
       "      <td>204.410</td>\n",
       "      <td>204.600</td>\n",
       "      <td>201.9200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>203.1601</td>\n",
       "      <td>159140400</td>\n",
       "      <td>205.91</td>\n",
       "      <td>205.980</td>\n",
       "      <td>202.93</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "      <td>204.19</td>\n",
       "      <td>...</td>\n",
       "      <td>205.90</td>\n",
       "      <td>146039400.0</td>\n",
       "      <td>204.010</td>\n",
       "      <td>206.160</td>\n",
       "      <td>203.9900</td>\n",
       "      <td>204.2500</td>\n",
       "      <td>157293400.0</td>\n",
       "      <td>206.400</td>\n",
       "      <td>206.420</td>\n",
       "      <td>203.5100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2279 rows Ã— 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Close/Last     Volume    Open     High     Low  day_of_week  day  month  \\\n",
       "237     475.3100  122283100  476.49  477.030  473.30            4   29     12   \n",
       "238     476.6900   77158120  476.88  477.550  476.26            3   28     12   \n",
       "239     476.5100   68000310  475.44  476.660  474.89            2   27     12   \n",
       "240     475.6500   55386950  474.07  476.580  473.99            1   26     12   \n",
       "241     473.6500   67160420  473.86  475.380  471.70            4   22     12   \n",
       "...          ...        ...     ...      ...     ...          ...  ...    ...   \n",
       "2511    197.9100  258867900  198.58  202.395  197.86            1   16     12   \n",
       "2512    199.5100  188920400  201.98  202.530  198.78            0   15     12   \n",
       "2513    200.8900  201983200  202.64  203.819  200.85            4   12     12   \n",
       "2514    204.1900  158542800  203.88  206.190  203.71            3   11     12   \n",
       "2515    203.1601  159140400  205.91  205.980  202.93            2   10     12   \n",
       "\n",
       "      year  prev1Close/Last  ...  prev19Close/Last  prev19Volume  prev19Open  \\\n",
       "237   2023           472.65  ...            491.27    61322750.0     487.730   \n",
       "238   2023           475.31  ...            487.41    76641610.0     487.590   \n",
       "239   2023           476.69  ...            488.03    72524990.0     487.575   \n",
       "240   2023           476.51  ...            485.39    81765040.0     487.810   \n",
       "241   2023           475.65  ...            484.86    49945300.0     484.010   \n",
       "...    ...              ...  ...               ...           ...         ...   \n",
       "2511  2014           201.79  ...            200.86   192551700.0     199.650   \n",
       "2512  2014           197.91  ...            202.08   214341100.0     204.120   \n",
       "2513  2014           199.51  ...            202.65   144043100.0     204.410   \n",
       "2514  2014           200.89  ...            204.25   157293400.0     206.400   \n",
       "2515  2014           204.19  ...            205.90   146039400.0     204.010   \n",
       "\n",
       "      prev19High  prev19Low  prev20Close/Last  prev20Volume  prev20Open  \\\n",
       "237      491.415   487.1700          490.8900    58618390.0     490.560   \n",
       "238      489.120   486.5400          491.2700    61322750.0     487.730   \n",
       "239      488.305   485.3900          487.4100    76641610.0     487.590   \n",
       "240      488.770   484.8819          488.0300    72524990.0     487.575   \n",
       "241      485.105   482.8900          485.3900    81765040.0     487.810   \n",
       "...          ...        ...               ...           ...         ...   \n",
       "2511     201.100   198.5700          199.0199   175383200.0     201.630   \n",
       "2512     205.480   200.5100          200.8600   192551700.0     199.650   \n",
       "2513     204.600   201.9200          202.0800   214341100.0     204.120   \n",
       "2514     206.420   203.5100          202.6500   144043100.0     204.410   \n",
       "2515     206.160   203.9900          204.2500   157293400.0     206.400   \n",
       "\n",
       "      prev20High  prev20Low  \n",
       "237      491.620   490.1100  \n",
       "238      491.415   487.1700  \n",
       "239      489.120   486.5400  \n",
       "240      488.305   485.3900  \n",
       "241      488.770   484.8819  \n",
       "...          ...        ...  \n",
       "2511     202.010   198.8800  \n",
       "2512     201.100   198.5700  \n",
       "2513     205.480   200.5100  \n",
       "2514     204.600   201.9200  \n",
       "2515     206.420   203.5100  \n",
       "\n",
       "[2279 rows x 109 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def is_up(row):\n",
    "    try:\n",
    "        # Ensure the next row exists before accessing\n",
    "        if row[\"Close/Last\"] + 1 < df.iloc[row[\"index\"] + 1][\"Close/Last\"]:\n",
    "            # Compare \"Close\" values\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return 0\n",
    "    \n",
    "# def is_down(row):\n",
    "#     try:\n",
    "#         # Ensure the next row exists before accessing\n",
    "#         if row[\"Close\"] -1 > df.iloc[row[\"index\"] + 1][\"Close\"]:\n",
    "#             # Compare \"Close\" values\n",
    "#             return 1\n",
    "#         else:\n",
    "#             return 0\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")\n",
    "#         return 0\n",
    "        \n",
    "df[\"NextDayUp1\"] = df.apply(is_up, axis=1)\n",
    "# df[\"NextDayDown1\"] = df.apply(is_down, axis=1)\n",
    "df\n",
    "\n",
    "\n",
    "y = df[df[\"year\"]!=2024]['NextDayUp1']\n",
    "X = df[df[\"year\"] != 2024].drop(['Date', \"NextDayUp1\", \"index\"], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=41)\n",
    "\n",
    "# fit scaler on training data\n",
    "norm = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# transform data\n",
    "X_train = norm.transform(X_train)\n",
    "X_test = norm.transform(X_test)\n",
    "\n",
    "hold_X = df[df[\"year\"] == 2024].drop(['Date', \"NextDayUp1\", \"index\"], axis=1)\n",
    "hold_y = df[df[\"year\"] == 2024]['NextDayUp1']\n",
    "hold_X = norm.transform(hold_X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def next(row):\n",
    "#     try:\n",
    "#         return df.iloc[row[\"index\"] + 1][\"Close/Last\"]\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")\n",
    "#         return 0\n",
    "    \n",
    "\n",
    "        \n",
    "# df[\"Next\"] = df.apply(next, axis=1)\n",
    "# # df[\"NextDayDown1\"] = df.apply(is_down, axis=1)\n",
    "# df\n",
    "\n",
    "\n",
    "# y = df[df[\"year\"]!=2024]['Next']\n",
    "# X = df[df[\"year\"]!=2024].drop(['Date', \"Next\", \"index\"], axis=1)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=41)\n",
    "\n",
    "# # fit scaler on training data\n",
    "# norm = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# # transform data\n",
    "# X_train = norm.transform(X_train)\n",
    "# X_test = norm.transform(X_test)\n",
    "\n",
    "# hold_X = df[df[\"year\"] == 2024].drop(['Date', \"Next\", \"index\"], axis=1)\n",
    "# hold_y = df[df[\"year\"] == 2024]['Next']\n",
    "# hold_X = norm.transform(hold_X)\n",
    "# X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      " dense_28 (Dense)            (None, 500)               55000     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 64)                32064     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89,177\n",
      "Trainable params: 89,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "19/19 [==============================] - 0s 12ms/step - loss: 0.2808 - mse: 0.2416 - val_loss: 0.2019 - val_mse: 0.2019\n",
      "Epoch 2/1000\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.2618 - mse: 0.2348 - val_loss: 0.2000 - val_mse: 0.2000\n",
      "Epoch 3/1000\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.2450 - mse: 0.2258 - val_loss: 0.1997 - val_mse: 0.1997\n",
      "Epoch 4/1000\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.2548 - mse: 0.2364 - val_loss: 0.2009 - val_mse: 0.2009\n",
      "Epoch 5/1000\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.2264 - mse: 0.2131 - val_loss: 0.2012 - val_mse: 0.2012\n",
      "Epoch 6/1000\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.2282 - mse: 0.2156 - val_loss: 0.2022 - val_mse: 0.2022\n",
      "Epoch 7/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2377 - mse: 0.2241 - val_loss: 0.2013 - val_mse: 0.2013\n",
      "Epoch 8/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2416 - mse: 0.2300 - val_loss: 0.1993 - val_mse: 0.1993\n",
      "Epoch 9/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2415 - mse: 0.2267 - val_loss: 0.1999 - val_mse: 0.1999\n",
      "Epoch 10/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2351 - mse: 0.2178 - val_loss: 0.2031 - val_mse: 0.2031\n",
      "Epoch 11/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2403 - mse: 0.2288 - val_loss: 0.1995 - val_mse: 0.1995\n",
      "Epoch 12/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2364 - mse: 0.2231 - val_loss: 0.2034 - val_mse: 0.2034\n",
      "Epoch 13/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2415 - mse: 0.2300 - val_loss: 0.1960 - val_mse: 0.1960\n",
      "Epoch 14/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2398 - mse: 0.2272 - val_loss: 0.1974 - val_mse: 0.1974\n",
      "Epoch 15/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2323 - mse: 0.2204 - val_loss: 0.2003 - val_mse: 0.2003\n",
      "Epoch 16/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2300 - mse: 0.2158 - val_loss: 0.1980 - val_mse: 0.1980\n",
      "Epoch 17/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2274 - mse: 0.2134 - val_loss: 0.2014 - val_mse: 0.2014\n",
      "Epoch 18/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2325 - mse: 0.2207 - val_loss: 0.1981 - val_mse: 0.1981\n",
      "Epoch 19/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2343 - mse: 0.2228 - val_loss: 0.2002 - val_mse: 0.2002\n",
      "Epoch 20/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2235 - mse: 0.2154 - val_loss: 0.2010 - val_mse: 0.2010\n",
      "Epoch 21/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2228 - mse: 0.2126 - val_loss: 0.1998 - val_mse: 0.1998\n",
      "Epoch 22/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2281 - mse: 0.2200 - val_loss: 0.1985 - val_mse: 0.1985\n",
      "Epoch 23/1000\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.2197 - mse: 0.2110 - val_loss: 0.1992 - val_mse: 0.1992\n",
      "Epoch 24/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2211 - mse: 0.2128 - val_loss: 0.1985 - val_mse: 0.1985\n",
      "Epoch 25/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2254 - mse: 0.2168 - val_loss: 0.1995 - val_mse: 0.1995\n",
      "Epoch 26/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2259 - mse: 0.2185 - val_loss: 0.2022 - val_mse: 0.2022\n",
      "Epoch 27/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2193 - mse: 0.2054 - val_loss: 0.2021 - val_mse: 0.2021\n",
      "Epoch 28/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2145 - mse: 0.2069 - val_loss: 0.2003 - val_mse: 0.2003\n",
      "Epoch 29/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2230 - mse: 0.2120 - val_loss: 0.2028 - val_mse: 0.2028\n",
      "Epoch 30/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2163 - mse: 0.2053 - val_loss: 0.2002 - val_mse: 0.2002\n",
      "Epoch 31/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2156 - mse: 0.2096 - val_loss: 0.2019 - val_mse: 0.2019\n",
      "Epoch 32/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2169 - mse: 0.2052 - val_loss: 0.1990 - val_mse: 0.1990\n",
      "Epoch 33/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2111 - mse: 0.2086 - val_loss: 0.1998 - val_mse: 0.1998\n",
      "Epoch 34/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2151 - mse: 0.2052 - val_loss: 0.2029 - val_mse: 0.2029\n",
      "Epoch 35/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2078 - mse: 0.2011 - val_loss: 0.2013 - val_mse: 0.2013\n",
      "Epoch 36/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2123 - mse: 0.2046 - val_loss: 0.2050 - val_mse: 0.2050\n",
      "Epoch 37/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2113 - mse: 0.2027 - val_loss: 0.2091 - val_mse: 0.2091\n",
      "Epoch 38/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2117 - mse: 0.2003 - val_loss: 0.2117 - val_mse: 0.2117\n",
      "Epoch 39/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2168 - mse: 0.2038 - val_loss: 0.2033 - val_mse: 0.2033\n",
      "Epoch 40/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2212 - mse: 0.2077 - val_loss: 0.2152 - val_mse: 0.2152\n",
      "Epoch 41/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2165 - mse: 0.2021 - val_loss: 0.2015 - val_mse: 0.2015\n",
      "Epoch 42/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2154 - mse: 0.2105 - val_loss: 0.1991 - val_mse: 0.1991\n",
      "Epoch 43/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2054 - mse: 0.1962 - val_loss: 0.2039 - val_mse: 0.2039\n",
      "Epoch 44/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2122 - mse: 0.2003 - val_loss: 0.2036 - val_mse: 0.2036\n",
      "Epoch 45/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2146 - mse: 0.2024 - val_loss: 0.2052 - val_mse: 0.2052\n",
      "Epoch 46/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2135 - mse: 0.2030 - val_loss: 0.2069 - val_mse: 0.2069\n",
      "Epoch 47/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2099 - mse: 0.2035 - val_loss: 0.2032 - val_mse: 0.2032\n",
      "Epoch 48/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2114 - mse: 0.1993 - val_loss: 0.2067 - val_mse: 0.2067\n",
      "Epoch 49/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2119 - mse: 0.2016 - val_loss: 0.2095 - val_mse: 0.2095\n",
      "Epoch 50/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2103 - mse: 0.2035 - val_loss: 0.2008 - val_mse: 0.2008\n",
      "Epoch 51/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2055 - mse: 0.1941 - val_loss: 0.2068 - val_mse: 0.2068\n",
      "Epoch 52/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2068 - mse: 0.1956 - val_loss: 0.2091 - val_mse: 0.2091\n",
      "Epoch 53/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2082 - mse: 0.1994 - val_loss: 0.2100 - val_mse: 0.2100\n",
      "Epoch 54/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2052 - mse: 0.1971 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 55/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2102 - mse: 0.2038 - val_loss: 0.2047 - val_mse: 0.2047\n",
      "Epoch 56/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2037 - mse: 0.1937 - val_loss: 0.2054 - val_mse: 0.2054\n",
      "Epoch 57/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2053 - mse: 0.1959 - val_loss: 0.2057 - val_mse: 0.2057\n",
      "Epoch 58/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1959 - mse: 0.1855 - val_loss: 0.2011 - val_mse: 0.2011\n",
      "Epoch 59/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2038 - mse: 0.1945 - val_loss: 0.2043 - val_mse: 0.2043\n",
      "Epoch 60/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2044 - mse: 0.1943 - val_loss: 0.2022 - val_mse: 0.2022\n",
      "Epoch 61/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1989 - mse: 0.1884 - val_loss: 0.2066 - val_mse: 0.2066\n",
      "Epoch 62/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2089 - mse: 0.2002 - val_loss: 0.2049 - val_mse: 0.2049\n",
      "Epoch 63/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2029 - mse: 0.1956 - val_loss: 0.2093 - val_mse: 0.2093\n",
      "Epoch 64/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1997 - mse: 0.1885 - val_loss: 0.2026 - val_mse: 0.2026\n",
      "Epoch 65/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2004 - mse: 0.1904 - val_loss: 0.2047 - val_mse: 0.2047\n",
      "Epoch 66/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2018 - mse: 0.1920 - val_loss: 0.2073 - val_mse: 0.2073\n",
      "Epoch 67/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1997 - mse: 0.1940 - val_loss: 0.2054 - val_mse: 0.2054\n",
      "Epoch 68/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2032 - mse: 0.1936 - val_loss: 0.2035 - val_mse: 0.2035\n",
      "Epoch 69/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.2058 - mse: 0.1977 - val_loss: 0.2029 - val_mse: 0.2029\n",
      "Epoch 70/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1969 - mse: 0.1848 - val_loss: 0.2025 - val_mse: 0.2025\n",
      "Epoch 71/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1918 - mse: 0.1874 - val_loss: 0.2040 - val_mse: 0.2040\n",
      "Epoch 72/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1941 - mse: 0.1908 - val_loss: 0.1996 - val_mse: 0.1996\n",
      "Epoch 73/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1943 - mse: 0.1905 - val_loss: 0.2009 - val_mse: 0.2009\n",
      "Epoch 74/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1924 - mse: 0.1811 - val_loss: 0.2013 - val_mse: 0.2013\n",
      "Epoch 75/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1975 - mse: 0.1880 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 76/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1950 - mse: 0.1844 - val_loss: 0.2065 - val_mse: 0.2065\n",
      "Epoch 77/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1919 - mse: 0.1833 - val_loss: 0.2025 - val_mse: 0.2025\n",
      "Epoch 78/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1945 - mse: 0.1862 - val_loss: 0.2044 - val_mse: 0.2044\n",
      "Epoch 79/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1885 - mse: 0.1812 - val_loss: 0.2006 - val_mse: 0.2006\n",
      "Epoch 80/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1937 - mse: 0.1882 - val_loss: 0.2001 - val_mse: 0.2001\n",
      "Epoch 81/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1978 - mse: 0.1874 - val_loss: 0.2007 - val_mse: 0.2007\n",
      "Epoch 82/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1961 - mse: 0.1849 - val_loss: 0.2067 - val_mse: 0.2067\n",
      "Epoch 83/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1937 - mse: 0.1848 - val_loss: 0.2054 - val_mse: 0.2054\n",
      "Epoch 84/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1952 - mse: 0.1888 - val_loss: 0.2036 - val_mse: 0.2036\n",
      "Epoch 85/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1892 - mse: 0.1801 - val_loss: 0.2108 - val_mse: 0.2108\n",
      "Epoch 86/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1906 - mse: 0.1840 - val_loss: 0.2059 - val_mse: 0.2059\n",
      "Epoch 87/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1904 - mse: 0.1820 - val_loss: 0.2100 - val_mse: 0.2100\n",
      "Epoch 88/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1879 - mse: 0.1839 - val_loss: 0.2067 - val_mse: 0.2067\n",
      "Epoch 89/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1832 - mse: 0.1747 - val_loss: 0.2063 - val_mse: 0.2063\n",
      "Epoch 90/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1941 - mse: 0.1854 - val_loss: 0.2085 - val_mse: 0.2085\n",
      "Epoch 91/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1849 - mse: 0.1802 - val_loss: 0.2052 - val_mse: 0.2052\n",
      "Epoch 92/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1946 - mse: 0.1793 - val_loss: 0.2075 - val_mse: 0.2075\n",
      "Epoch 93/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1887 - mse: 0.1839 - val_loss: 0.2051 - val_mse: 0.2051\n",
      "Epoch 94/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1917 - mse: 0.1810 - val_loss: 0.2066 - val_mse: 0.2066\n",
      "Epoch 95/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1908 - mse: 0.1835 - val_loss: 0.2081 - val_mse: 0.2081\n",
      "Epoch 96/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1896 - mse: 0.1839 - val_loss: 0.2077 - val_mse: 0.2077\n",
      "Epoch 97/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1917 - mse: 0.1824 - val_loss: 0.1997 - val_mse: 0.1997\n",
      "Epoch 98/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1852 - mse: 0.1773 - val_loss: 0.2005 - val_mse: 0.2005\n",
      "Epoch 99/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1871 - mse: 0.1786 - val_loss: 0.2000 - val_mse: 0.2000\n",
      "Epoch 100/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1896 - mse: 0.1818 - val_loss: 0.2037 - val_mse: 0.2037\n",
      "Epoch 101/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1881 - mse: 0.1815 - val_loss: 0.2083 - val_mse: 0.2083\n",
      "Epoch 102/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1899 - mse: 0.1825 - val_loss: 0.2033 - val_mse: 0.2033\n",
      "Epoch 103/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1893 - mse: 0.1784 - val_loss: 0.2048 - val_mse: 0.2048\n",
      "Epoch 104/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1809 - mse: 0.1751 - val_loss: 0.2031 - val_mse: 0.2031\n",
      "Epoch 105/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1813 - mse: 0.1726 - val_loss: 0.2069 - val_mse: 0.2069\n",
      "Epoch 106/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1966 - mse: 0.1829 - val_loss: 0.2196 - val_mse: 0.2196\n",
      "Epoch 107/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1879 - mse: 0.1757 - val_loss: 0.2115 - val_mse: 0.2115\n",
      "Epoch 108/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1870 - mse: 0.1822 - val_loss: 0.2068 - val_mse: 0.2068\n",
      "Epoch 109/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1876 - mse: 0.1782 - val_loss: 0.2105 - val_mse: 0.2105\n",
      "Epoch 110/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1817 - mse: 0.1717 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 111/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1818 - mse: 0.1748 - val_loss: 0.2015 - val_mse: 0.2015\n",
      "Epoch 112/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1839 - mse: 0.1763 - val_loss: 0.2121 - val_mse: 0.2121\n",
      "Epoch 113/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1883 - mse: 0.1793 - val_loss: 0.2091 - val_mse: 0.2091\n",
      "Epoch 114/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1774 - mse: 0.1776 - val_loss: 0.2041 - val_mse: 0.2041\n",
      "Epoch 115/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1813 - mse: 0.1731 - val_loss: 0.2020 - val_mse: 0.2020\n",
      "Epoch 116/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1836 - mse: 0.1765 - val_loss: 0.2090 - val_mse: 0.2090\n",
      "Epoch 117/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1815 - mse: 0.1711 - val_loss: 0.2106 - val_mse: 0.2106\n",
      "Epoch 118/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1803 - mse: 0.1764 - val_loss: 0.2118 - val_mse: 0.2118\n",
      "Epoch 119/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1808 - mse: 0.1765 - val_loss: 0.2070 - val_mse: 0.2070\n",
      "Epoch 120/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1836 - mse: 0.1772 - val_loss: 0.2042 - val_mse: 0.2042\n",
      "Epoch 121/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1764 - mse: 0.1686 - val_loss: 0.2114 - val_mse: 0.2114\n",
      "Epoch 122/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1808 - mse: 0.1754 - val_loss: 0.2153 - val_mse: 0.2153\n",
      "Epoch 123/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1816 - mse: 0.1752 - val_loss: 0.2083 - val_mse: 0.2083\n",
      "Epoch 124/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1794 - mse: 0.1761 - val_loss: 0.2095 - val_mse: 0.2095\n",
      "Epoch 125/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1862 - mse: 0.1831 - val_loss: 0.2079 - val_mse: 0.2079\n",
      "Epoch 126/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1847 - mse: 0.1776 - val_loss: 0.2112 - val_mse: 0.2112\n",
      "Epoch 127/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1739 - mse: 0.1674 - val_loss: 0.2098 - val_mse: 0.2098\n",
      "Epoch 128/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1837 - mse: 0.1714 - val_loss: 0.2170 - val_mse: 0.2170\n",
      "Epoch 129/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1775 - mse: 0.1704 - val_loss: 0.2068 - val_mse: 0.2068\n",
      "Epoch 130/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1778 - mse: 0.1723 - val_loss: 0.2066 - val_mse: 0.2066\n",
      "Epoch 131/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1740 - mse: 0.1701 - val_loss: 0.2083 - val_mse: 0.2083\n",
      "Epoch 132/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1792 - mse: 0.1702 - val_loss: 0.2185 - val_mse: 0.2185\n",
      "Epoch 133/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1708 - mse: 0.1681 - val_loss: 0.2257 - val_mse: 0.2257\n",
      "Epoch 134/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1735 - mse: 0.1714 - val_loss: 0.2162 - val_mse: 0.2162\n",
      "Epoch 135/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1719 - mse: 0.1677 - val_loss: 0.2116 - val_mse: 0.2116\n",
      "Epoch 136/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1736 - mse: 0.1715 - val_loss: 0.2105 - val_mse: 0.2105\n",
      "Epoch 137/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1773 - mse: 0.1712 - val_loss: 0.2140 - val_mse: 0.2140\n",
      "Epoch 138/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1767 - mse: 0.1684 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 139/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1745 - mse: 0.1696 - val_loss: 0.2158 - val_mse: 0.2158\n",
      "Epoch 140/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1789 - mse: 0.1735 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 141/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1720 - mse: 0.1681 - val_loss: 0.2289 - val_mse: 0.2289\n",
      "Epoch 142/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1777 - mse: 0.1713 - val_loss: 0.2206 - val_mse: 0.2206\n",
      "Epoch 143/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1667 - mse: 0.1622 - val_loss: 0.2188 - val_mse: 0.2188\n",
      "Epoch 144/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1767 - mse: 0.1710 - val_loss: 0.2156 - val_mse: 0.2156\n",
      "Epoch 145/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1708 - mse: 0.1671 - val_loss: 0.2152 - val_mse: 0.2152\n",
      "Epoch 146/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1660 - mse: 0.1627 - val_loss: 0.2162 - val_mse: 0.2162\n",
      "Epoch 147/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1735 - mse: 0.1681 - val_loss: 0.2200 - val_mse: 0.2200\n",
      "Epoch 148/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1718 - mse: 0.1690 - val_loss: 0.2219 - val_mse: 0.2219\n",
      "Epoch 149/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1620 - mse: 0.1592 - val_loss: 0.2145 - val_mse: 0.2145\n",
      "Epoch 150/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1743 - mse: 0.1675 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 151/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1691 - mse: 0.1612 - val_loss: 0.2131 - val_mse: 0.2131\n",
      "Epoch 152/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1671 - mse: 0.1633 - val_loss: 0.2155 - val_mse: 0.2155\n",
      "Epoch 153/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1676 - mse: 0.1601 - val_loss: 0.2111 - val_mse: 0.2111\n",
      "Epoch 154/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1753 - mse: 0.1674 - val_loss: 0.2127 - val_mse: 0.2127\n",
      "Epoch 155/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1740 - mse: 0.1694 - val_loss: 0.2138 - val_mse: 0.2138\n",
      "Epoch 156/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1701 - mse: 0.1673 - val_loss: 0.2209 - val_mse: 0.2209\n",
      "Epoch 157/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1756 - mse: 0.1672 - val_loss: 0.2150 - val_mse: 0.2150\n",
      "Epoch 158/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1704 - mse: 0.1638 - val_loss: 0.2227 - val_mse: 0.2227\n",
      "Epoch 159/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1689 - mse: 0.1639 - val_loss: 0.2162 - val_mse: 0.2162\n",
      "Epoch 160/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1625 - mse: 0.1572 - val_loss: 0.2246 - val_mse: 0.2246\n",
      "Epoch 161/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1689 - mse: 0.1651 - val_loss: 0.2202 - val_mse: 0.2202\n",
      "Epoch 162/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1717 - mse: 0.1668 - val_loss: 0.2215 - val_mse: 0.2215\n",
      "Epoch 163/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1735 - mse: 0.1623 - val_loss: 0.2186 - val_mse: 0.2186\n",
      "Epoch 164/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1662 - mse: 0.1681 - val_loss: 0.2148 - val_mse: 0.2148\n",
      "Epoch 165/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1682 - mse: 0.1651 - val_loss: 0.2199 - val_mse: 0.2199\n",
      "Epoch 166/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1629 - mse: 0.1607 - val_loss: 0.2163 - val_mse: 0.2163\n",
      "Epoch 167/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1687 - mse: 0.1599 - val_loss: 0.2115 - val_mse: 0.2115\n",
      "Epoch 168/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1664 - mse: 0.1593 - val_loss: 0.2110 - val_mse: 0.2110\n",
      "Epoch 169/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1626 - mse: 0.1588 - val_loss: 0.2104 - val_mse: 0.2104\n",
      "Epoch 170/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1718 - mse: 0.1688 - val_loss: 0.2085 - val_mse: 0.2085\n",
      "Epoch 171/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1571 - mse: 0.1515 - val_loss: 0.2109 - val_mse: 0.2109\n",
      "Epoch 172/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1612 - mse: 0.1591 - val_loss: 0.2191 - val_mse: 0.2191\n",
      "Epoch 173/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1607 - mse: 0.1570 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 174/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1618 - mse: 0.1567 - val_loss: 0.2243 - val_mse: 0.2243\n",
      "Epoch 175/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1616 - mse: 0.1605 - val_loss: 0.2169 - val_mse: 0.2169\n",
      "Epoch 176/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1597 - mse: 0.1542 - val_loss: 0.2223 - val_mse: 0.2223\n",
      "Epoch 177/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1621 - mse: 0.1560 - val_loss: 0.2202 - val_mse: 0.2202\n",
      "Epoch 178/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1633 - mse: 0.1610 - val_loss: 0.2177 - val_mse: 0.2177\n",
      "Epoch 179/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1637 - mse: 0.1618 - val_loss: 0.2206 - val_mse: 0.2206\n",
      "Epoch 180/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1585 - mse: 0.1563 - val_loss: 0.2190 - val_mse: 0.2190\n",
      "Epoch 181/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1570 - mse: 0.1585 - val_loss: 0.2129 - val_mse: 0.2129\n",
      "Epoch 182/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1624 - mse: 0.1566 - val_loss: 0.2158 - val_mse: 0.2158\n",
      "Epoch 183/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1626 - mse: 0.1577 - val_loss: 0.2177 - val_mse: 0.2177\n",
      "Epoch 184/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1638 - mse: 0.1638 - val_loss: 0.2216 - val_mse: 0.2216\n",
      "Epoch 185/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1604 - mse: 0.1573 - val_loss: 0.2197 - val_mse: 0.2197\n",
      "Epoch 186/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1578 - mse: 0.1564 - val_loss: 0.2236 - val_mse: 0.2236\n",
      "Epoch 187/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1610 - mse: 0.1592 - val_loss: 0.2272 - val_mse: 0.2272\n",
      "Epoch 188/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1549 - mse: 0.1541 - val_loss: 0.2148 - val_mse: 0.2148\n",
      "Epoch 189/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1639 - mse: 0.1590 - val_loss: 0.2243 - val_mse: 0.2243\n",
      "Epoch 190/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1582 - mse: 0.1596 - val_loss: 0.2234 - val_mse: 0.2234\n",
      "Epoch 191/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1573 - mse: 0.1527 - val_loss: 0.2205 - val_mse: 0.2205\n",
      "Epoch 192/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1642 - mse: 0.1593 - val_loss: 0.2215 - val_mse: 0.2215\n",
      "Epoch 193/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1591 - mse: 0.1561 - val_loss: 0.2196 - val_mse: 0.2196\n",
      "Epoch 194/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1593 - mse: 0.1572 - val_loss: 0.2303 - val_mse: 0.2303\n",
      "Epoch 195/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1650 - mse: 0.1608 - val_loss: 0.2233 - val_mse: 0.2233\n",
      "Epoch 196/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1593 - mse: 0.1570 - val_loss: 0.2279 - val_mse: 0.2279\n",
      "Epoch 197/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1622 - mse: 0.1605 - val_loss: 0.2178 - val_mse: 0.2178\n",
      "Epoch 198/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1579 - mse: 0.1514 - val_loss: 0.2218 - val_mse: 0.2218\n",
      "Epoch 199/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1558 - mse: 0.1530 - val_loss: 0.2259 - val_mse: 0.2259\n",
      "Epoch 200/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1542 - mse: 0.1530 - val_loss: 0.2315 - val_mse: 0.2315\n",
      "Epoch 201/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1560 - mse: 0.1545 - val_loss: 0.2234 - val_mse: 0.2234\n",
      "Epoch 202/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1602 - mse: 0.1589 - val_loss: 0.2308 - val_mse: 0.2308\n",
      "Epoch 203/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1565 - mse: 0.1522 - val_loss: 0.2340 - val_mse: 0.2340\n",
      "Epoch 204/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1574 - mse: 0.1590 - val_loss: 0.2244 - val_mse: 0.2244\n",
      "Epoch 205/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1592 - mse: 0.1563 - val_loss: 0.2290 - val_mse: 0.2290\n",
      "Epoch 206/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1613 - mse: 0.1576 - val_loss: 0.2230 - val_mse: 0.2230\n",
      "Epoch 207/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1632 - mse: 0.1586 - val_loss: 0.2242 - val_mse: 0.2242\n",
      "Epoch 208/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1626 - mse: 0.1605 - val_loss: 0.2321 - val_mse: 0.2321\n",
      "Epoch 209/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1560 - mse: 0.1559 - val_loss: 0.2234 - val_mse: 0.2234\n",
      "Epoch 210/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1550 - mse: 0.1535 - val_loss: 0.2251 - val_mse: 0.2251\n",
      "Epoch 211/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1539 - mse: 0.1510 - val_loss: 0.2176 - val_mse: 0.2176\n",
      "Epoch 212/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1495 - mse: 0.1491 - val_loss: 0.2246 - val_mse: 0.2246\n",
      "Epoch 213/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1542 - mse: 0.1569 - val_loss: 0.2230 - val_mse: 0.2230\n",
      "Epoch 214/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1579 - mse: 0.1565 - val_loss: 0.2252 - val_mse: 0.2252\n",
      "Epoch 215/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1572 - mse: 0.1556 - val_loss: 0.2224 - val_mse: 0.2224\n",
      "Epoch 216/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1451 - mse: 0.1488 - val_loss: 0.2194 - val_mse: 0.2194\n",
      "Epoch 217/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1487 - mse: 0.1479 - val_loss: 0.2217 - val_mse: 0.2217\n",
      "Epoch 218/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1628 - mse: 0.1606 - val_loss: 0.2149 - val_mse: 0.2149\n",
      "Epoch 219/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1489 - mse: 0.1505 - val_loss: 0.2217 - val_mse: 0.2217\n",
      "Epoch 220/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1505 - mse: 0.1461 - val_loss: 0.2251 - val_mse: 0.2251\n",
      "Epoch 221/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1561 - mse: 0.1552 - val_loss: 0.2313 - val_mse: 0.2313\n",
      "Epoch 222/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1546 - mse: 0.1529 - val_loss: 0.2283 - val_mse: 0.2283\n",
      "Epoch 223/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1480 - mse: 0.1490 - val_loss: 0.2249 - val_mse: 0.2249\n",
      "Epoch 224/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1445 - mse: 0.1403 - val_loss: 0.2300 - val_mse: 0.2300\n",
      "Epoch 225/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1524 - mse: 0.1519 - val_loss: 0.2317 - val_mse: 0.2317\n",
      "Epoch 226/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1549 - mse: 0.1538 - val_loss: 0.2403 - val_mse: 0.2403\n",
      "Epoch 227/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1470 - mse: 0.1440 - val_loss: 0.2333 - val_mse: 0.2333\n",
      "Epoch 228/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1563 - mse: 0.1530 - val_loss: 0.2287 - val_mse: 0.2287\n",
      "Epoch 229/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1461 - mse: 0.1445 - val_loss: 0.2341 - val_mse: 0.2341\n",
      "Epoch 230/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1476 - mse: 0.1463 - val_loss: 0.2339 - val_mse: 0.2339\n",
      "Epoch 231/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1461 - mse: 0.1487 - val_loss: 0.2190 - val_mse: 0.2190\n",
      "Epoch 232/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1424 - mse: 0.1383 - val_loss: 0.2199 - val_mse: 0.2199\n",
      "Epoch 233/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1567 - mse: 0.1560 - val_loss: 0.2303 - val_mse: 0.2303\n",
      "Epoch 234/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1455 - mse: 0.1473 - val_loss: 0.2286 - val_mse: 0.2286\n",
      "Epoch 235/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1455 - mse: 0.1420 - val_loss: 0.2280 - val_mse: 0.2280\n",
      "Epoch 236/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1457 - mse: 0.1480 - val_loss: 0.2242 - val_mse: 0.2242\n",
      "Epoch 237/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1406 - mse: 0.1404 - val_loss: 0.2230 - val_mse: 0.2230\n",
      "Epoch 238/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1460 - mse: 0.1483 - val_loss: 0.2364 - val_mse: 0.2364\n",
      "Epoch 239/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1517 - mse: 0.1491 - val_loss: 0.2379 - val_mse: 0.2379\n",
      "Epoch 240/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1494 - mse: 0.1511 - val_loss: 0.2310 - val_mse: 0.2310\n",
      "Epoch 241/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1508 - mse: 0.1458 - val_loss: 0.2308 - val_mse: 0.2308\n",
      "Epoch 242/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1509 - mse: 0.1469 - val_loss: 0.2337 - val_mse: 0.2337\n",
      "Epoch 243/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1495 - mse: 0.1530 - val_loss: 0.2268 - val_mse: 0.2268\n",
      "Epoch 244/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1495 - mse: 0.1487 - val_loss: 0.2363 - val_mse: 0.2363\n",
      "Epoch 245/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1394 - mse: 0.1412 - val_loss: 0.2368 - val_mse: 0.2368\n",
      "Epoch 246/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1351 - mse: 0.1362 - val_loss: 0.2248 - val_mse: 0.2248\n",
      "Epoch 247/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1423 - mse: 0.1459 - val_loss: 0.2211 - val_mse: 0.2211\n",
      "Epoch 248/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1348 - mse: 0.1325 - val_loss: 0.2287 - val_mse: 0.2287\n",
      "Epoch 249/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1407 - mse: 0.1350 - val_loss: 0.2314 - val_mse: 0.2314\n",
      "Epoch 250/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1399 - mse: 0.1395 - val_loss: 0.2305 - val_mse: 0.2305\n",
      "Epoch 251/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1401 - mse: 0.1419 - val_loss: 0.2279 - val_mse: 0.2279\n",
      "Epoch 252/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1363 - mse: 0.1336 - val_loss: 0.2297 - val_mse: 0.2297\n",
      "Epoch 253/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1553 - mse: 0.1542 - val_loss: 0.2284 - val_mse: 0.2284\n",
      "Epoch 254/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1395 - mse: 0.1389 - val_loss: 0.2216 - val_mse: 0.2216\n",
      "Epoch 255/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1441 - mse: 0.1408 - val_loss: 0.2308 - val_mse: 0.2308\n",
      "Epoch 256/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1489 - mse: 0.1501 - val_loss: 0.2384 - val_mse: 0.2384\n",
      "Epoch 257/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1405 - mse: 0.1392 - val_loss: 0.2500 - val_mse: 0.2500\n",
      "Epoch 258/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1445 - mse: 0.1445 - val_loss: 0.2322 - val_mse: 0.2322\n",
      "Epoch 259/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1473 - mse: 0.1498 - val_loss: 0.2303 - val_mse: 0.2303\n",
      "Epoch 260/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1438 - mse: 0.1410 - val_loss: 0.2332 - val_mse: 0.2332\n",
      "Epoch 261/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1508 - mse: 0.1494 - val_loss: 0.2368 - val_mse: 0.2368\n",
      "Epoch 262/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1386 - mse: 0.1372 - val_loss: 0.2338 - val_mse: 0.2338\n",
      "Epoch 263/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1432 - mse: 0.1445 - val_loss: 0.2375 - val_mse: 0.2375\n",
      "Epoch 264/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1425 - mse: 0.1396 - val_loss: 0.2186 - val_mse: 0.2186\n",
      "Epoch 265/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1505 - mse: 0.1478 - val_loss: 0.2183 - val_mse: 0.2183\n",
      "Epoch 266/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1478 - mse: 0.1481 - val_loss: 0.2238 - val_mse: 0.2238\n",
      "Epoch 267/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1418 - mse: 0.1407 - val_loss: 0.2301 - val_mse: 0.2301\n",
      "Epoch 268/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1371 - mse: 0.1361 - val_loss: 0.2406 - val_mse: 0.2406\n",
      "Epoch 269/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1490 - mse: 0.1511 - val_loss: 0.2449 - val_mse: 0.2449\n",
      "Epoch 270/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1387 - mse: 0.1397 - val_loss: 0.2299 - val_mse: 0.2299\n",
      "Epoch 271/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1417 - mse: 0.1414 - val_loss: 0.2353 - val_mse: 0.2353\n",
      "Epoch 272/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1389 - mse: 0.1359 - val_loss: 0.2347 - val_mse: 0.2347\n",
      "Epoch 273/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1354 - mse: 0.1376 - val_loss: 0.2348 - val_mse: 0.2348\n",
      "Epoch 274/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1364 - mse: 0.1383 - val_loss: 0.2268 - val_mse: 0.2268\n",
      "Epoch 275/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1387 - mse: 0.1398 - val_loss: 0.2312 - val_mse: 0.2312\n",
      "Epoch 276/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1366 - mse: 0.1373 - val_loss: 0.2268 - val_mse: 0.2268\n",
      "Epoch 277/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1376 - mse: 0.1384 - val_loss: 0.2301 - val_mse: 0.2301\n",
      "Epoch 278/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1363 - mse: 0.1344 - val_loss: 0.2298 - val_mse: 0.2298\n",
      "Epoch 279/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1370 - mse: 0.1386 - val_loss: 0.2299 - val_mse: 0.2299\n",
      "Epoch 280/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1336 - mse: 0.1374 - val_loss: 0.2292 - val_mse: 0.2292\n",
      "Epoch 281/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1399 - mse: 0.1415 - val_loss: 0.2419 - val_mse: 0.2419\n",
      "Epoch 282/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1323 - mse: 0.1309 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 283/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1295 - mse: 0.1313 - val_loss: 0.2400 - val_mse: 0.2400\n",
      "Epoch 284/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1335 - mse: 0.1376 - val_loss: 0.2385 - val_mse: 0.2385\n",
      "Epoch 285/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1385 - mse: 0.1401 - val_loss: 0.2380 - val_mse: 0.2380\n",
      "Epoch 286/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1444 - mse: 0.1448 - val_loss: 0.2401 - val_mse: 0.2401\n",
      "Epoch 287/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1442 - mse: 0.1407 - val_loss: 0.2299 - val_mse: 0.2299\n",
      "Epoch 288/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1384 - mse: 0.1449 - val_loss: 0.2327 - val_mse: 0.2327\n",
      "Epoch 289/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1363 - mse: 0.1395 - val_loss: 0.2288 - val_mse: 0.2288\n",
      "Epoch 290/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1341 - mse: 0.1360 - val_loss: 0.2253 - val_mse: 0.2253\n",
      "Epoch 291/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1336 - mse: 0.1346 - val_loss: 0.2355 - val_mse: 0.2355\n",
      "Epoch 292/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1336 - mse: 0.1368 - val_loss: 0.2424 - val_mse: 0.2424\n",
      "Epoch 293/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1321 - mse: 0.1319 - val_loss: 0.2335 - val_mse: 0.2335\n",
      "Epoch 294/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1389 - mse: 0.1346 - val_loss: 0.2314 - val_mse: 0.2314\n",
      "Epoch 295/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1368 - mse: 0.1396 - val_loss: 0.2415 - val_mse: 0.2415\n",
      "Epoch 296/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1362 - mse: 0.1393 - val_loss: 0.2344 - val_mse: 0.2344\n",
      "Epoch 297/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1255 - mse: 0.1250 - val_loss: 0.2284 - val_mse: 0.2284\n",
      "Epoch 298/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1370 - mse: 0.1400 - val_loss: 0.2367 - val_mse: 0.2367\n",
      "Epoch 299/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1342 - mse: 0.1326 - val_loss: 0.2316 - val_mse: 0.2316\n",
      "Epoch 300/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1320 - mse: 0.1341 - val_loss: 0.2339 - val_mse: 0.2339\n",
      "Epoch 301/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1375 - mse: 0.1410 - val_loss: 0.2205 - val_mse: 0.2205\n",
      "Epoch 302/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1275 - mse: 0.1283 - val_loss: 0.2270 - val_mse: 0.2270\n",
      "Epoch 303/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1313 - mse: 0.1307 - val_loss: 0.2302 - val_mse: 0.2302\n",
      "Epoch 304/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1340 - mse: 0.1348 - val_loss: 0.2314 - val_mse: 0.2314\n",
      "Epoch 305/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1283 - mse: 0.1291 - val_loss: 0.2447 - val_mse: 0.2447\n",
      "Epoch 306/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1369 - mse: 0.1336 - val_loss: 0.2352 - val_mse: 0.2352\n",
      "Epoch 307/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1291 - mse: 0.1323 - val_loss: 0.2291 - val_mse: 0.2291\n",
      "Epoch 308/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1311 - mse: 0.1349 - val_loss: 0.2318 - val_mse: 0.2318\n",
      "Epoch 309/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1305 - mse: 0.1320 - val_loss: 0.2205 - val_mse: 0.2205\n",
      "Epoch 310/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1293 - mse: 0.1298 - val_loss: 0.2214 - val_mse: 0.2214\n",
      "Epoch 311/1000\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1280 - mse: 0.1293 - val_loss: 0.2268 - val_mse: 0.2268\n",
      "Epoch 312/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1382 - mse: 0.1415 - val_loss: 0.2340 - val_mse: 0.2340\n",
      "Epoch 313/1000\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.1290 - mse: 0.1303 - val_loss: 0.2375 - val_mse: 0.2375\n"
     ]
    }
   ],
   "source": [
    "num_features = len(X_train[0])\n",
    "\n",
    "# model.add(Dropout(.5))\n",
    "\n",
    "model = Sequential([\n",
    "  Dense(500, activation='relu', input_dim=num_features),\n",
    "  Dropout(.4),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dropout(.4),\n",
    "  Dense(32, activation='relu'),\n",
    "  Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0003)\n",
    "model.compile(loss='mse', optimizer=opt, metrics=['mse'])\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_mse', patience=300, mode='min')\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=1000, validation_split=.35, batch_size=40, callbacks=[early_stop], shuffle=False, class_weight=class_weights_dict)\n",
    "hist = pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 636us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = (model.predict(X_test) > 0.5).astype(np.int64)\n",
    "\n",
    "pred = pd.DataFrame(predictions,columns=['predictions'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6798245614035088\n",
      "0.5480769230769231\n",
      "0.43291139240506327\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, pred[\"predictions\"])\n",
    "recall = recall_score(y_test, pred[\"predictions\"])\n",
    "precision = precision_score(y_test, pred[\"predictions\"])\n",
    "print(accuracy)\n",
    "print(recall)\n",
    "print(precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 288us/step\n",
      "0.6371308016877637\n",
      "0.323943661971831\n",
      "0.3770491803278688\n"
     ]
    }
   ],
   "source": [
    "predictions = (model.predict(hold_X) > 0.5).astype(np.int64)\n",
    "\n",
    "pred = pd.DataFrame(predictions,columns=['predictions'])\n",
    "accuracy = accuracy_score(hold_y, pred[\"predictions\"])\n",
    "recall = recall_score(hold_y, pred[\"predictions\"])\n",
    "precision = precision_score(hold_y, pred[\"predictions\"])\n",
    "print(accuracy)\n",
    "print(recall)\n",
    "print(precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
